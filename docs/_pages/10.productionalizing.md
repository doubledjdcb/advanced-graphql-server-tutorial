---
layout: "step"
title: Production
nav_order: 10
permalink: /tutorial/production/
---

# Getting our GraphQL Server Ready for Production

In this tutorial we'll learn how to take our GraphQL server and prepare it for deployment into production. With the popularity of container services, we will be
utilizing Docker so our server can be deployed with a myriad of deployment orchestration tools including Kubernetes.

## Graceful Shutdowns

Some of the goals with deployment orchestration is the ability to deploy often and at scale as well as to scale up and down deployed instances to dynamically
manage load. What this means to us is that it is very likely that instances of our GraphQL server will be shut down frequently. Considering the nature of our
GraphQL service, we are going to want to add a bit of control over that process to prevent the server from exiting while it is in the middle of long query or
data resolution.

Node.js gives us the ability to intercept and handle various process [signal events](https://nodejs.org/api/process.html#process_signal_events). When a process
is terminated via Docker, a SIGTERM event is dispatched. Let's update our server to listen for that event and properly handle the event.

The first thing that we'll want to do is handle keep-alive session from HTTP1.1 connections. We'll add some middleware to our express server that will respond
with a appropriate response stating that the connection will be terminated. In this way, the client can create a new connection to a different instance of the
GraphQL server and continue on.

<pre><code class="language-javascript">
// File: src/index.js
...
let isGracefullyClosing = false;
app.use((req, res, next) => {
  // Terminate 'keep-alive' sessions
  if (isGracefullyClosing) {
    res.setHeader('Connection', 'close');
    res.send(502, 'Service is restarting');
  } else {
    next();
  }
});
...
</code></pre>
<br />

Next we'll write a process event handler. The purpose of this handler will be to restrict new connections from being made while completing the tasks of any
current connections. Our MySQL and MongoDB data-connector classes included methods to gracefully close all connections, so we can reference those methods in our
process event handler. Finally, once all express connection responses are complete and database connections closed, we'll tell the node process to exit with a
success code of 0. We'll also create a timeout function that will force the process to exit if there is a problem closing any connections. In this example, we
have set that timeout for one minute, but you should adjust for what is appropriate for your application.

Once the process handler is written, we'll call it when the node process receives the SIGTERM event.

<pre><code class="language-javascript">
// File: src/index.js
...
import MySQLConnector from './data-connectors/mysql';
import MongoConnector from './data-connectors/mongodb';
...
const gracefulShutdown = async () => {
  isGracefullyClosing = true;
  // Stop accepting new connections
  console.log('Stopping HTTP Server');
  httpServer.close(async () => {
    // Close any other open connections (i.e. database)
    const closeMySql = MySQLConnector.closeAllConnections();
    const closeMongo = MongoConnector.closeConnection();
    await closeMySql;
    await closeMongo;
    console.log('All connections closed');
    process.exit(0);
  });

  setTimeout(() => {
    console.error('Unable to close all connections. Forcing process to exit.');
    process.exit(1);
  }, 60 * 1000);
};

process.on('SIGTERM', gracefulShutdown);
</code></pre>
<br />

We'll also need to handle closing out web socket connections. To do this we need to update the <code class="language-javascript">subscriptions.onConnect</code>
property for our ApolloServer and tell the web socket that was created to close on SIGTERM events.

<pre><code class="language-javascript">
// File: src/index.js
const server = new ApolloServer({
  schema,
  context,
  subscriptions: {
    onConnect: async (connectionParams, <mark>webSocket</mark>) => {
      <mark>process.on('SIGTERM', webSocket.close);</mark>
      if (connectionParams.authToken) {
        const currentUser = await getUserFromToken(connectionParams.authToken);
        return { currentUser };
      }
      return {};
    },
  },
});
</code></pre>

### Notes on configuring a client

The server will now gracefully handle shutdowns, but you'll want to be certain that the client is configured to gracefully handle them as well, particularly
subscriptions. When configuring your subscription client, you'll want to make certain that you enable any auto-reconnect feature that may exist. Instructions
for doing this with Apollo Client can be found [here](https://www.apollographql.com/docs/react/advanced/subscriptions.html#subscriptions-client).

## Create the Docker image

Before we can deploy our docker image to our orchestration system, we need to create it. At the base of the project we'll create a Dockerfile. Docker creates
many different [official docker images for node](https://hub.docker.com/_/node/). I like to use alpine linux and install only what I need into my container.

First we'll set a working directory and copy over just the files we need to install our server dependencies. Since our dependencies change infrequently, this
will allow us to utilize the Docker build caching system to create much faster subsequent builds.

Next, in a single RUN command, we'll install the dependencies required for the node-rdkafka library, install all of our npm package dependencies and remove
anything that was installed to build node-rdkafka when is not needed at runtime. We do this all in a single RUN command to reduce the size of the final docker
layer and subsequently, the final image size.

With all of our dependencies installed, we can now copy over the source code and our .babelrc file which are needed to build our GraphQL server. Once built, we
can prune out our dev dependencies and remove the source.

<pre><code class="language-docker">
FROM node:10.15.0-alpine

WORKDIR /app
COPY package-lock.json .
COPY package.json .

RUN apk --no-cache add \
      ca-certificates \
      lz4-dev \
      musl-dev \
      cyrus-sasl-dev \
      openssl-dev \
      python \
  && \
    apk add --no-cache --virtual .build-deps \
      bash \
      g++ \
      gcc \
      zlib-dev \
      libc-dev \
      make \
      bsd-compat-headers \
      py-setuptools \
      bash \
  && \
    npm ci \
  && \
    apk del .build-deps

COPY .babelrc .
COPY src src

RUN npm run build
RUN npm --prune --production && rm -rf src

CMD npm start
</code></pre>
<br />

We can now build a docker image called "graphql-tutorial" locally by executing <code class="language-shell">docker build -t graphql-tutorial:1.0.0 .</code> in
our command prompt.

### Test the docker image

Since we already have a docker cluster running that we have been starting with our <code class="language-shell">npm run data-start</code> command, let's start
our new docker image and connect it to the docker network created for that cluster. Since our app will now be running in a docker environment, the connection
parameters for our databases will be slightly different. To handle this, we'll create a new .env-docker file with the same parameters as the current .env file,
but with the new connection values.

<pre><code class="language-shell">
# File: .env-docker
MYSQL_HOST=mysql
MYSQL_USER=graphql
MYSQL_PASSWORD=graphqlpw
MYSQL_DATABASE=central-library
MYSQL_CONNECTION_POOL_LIMIT=10
MYSQL_TIMEZONE=Z
MONGO_URL=mongodb://mongo:27017
MONGO_USER=graphql
MONGO_PASSWORD=graphqlpw
MONGO_DATABASE=central-library
MONGO_CONNECTION_POOL_SIZE=10
KAFKA_BROKERS=kafka1:19092
PUBLIC_JWT_KEY_URL=http://jwt:4080/publickey
</code></pre>

The name of the network that was specified in our docker-compose file was "advanced-graphql-tutorial" so when starting our docker container, we'll need to
reference this network.

Now we can simply issue the following command in our shell to start a new docker container from our new image, pass the environment variables from our
.env-docker file to the running container, attach the container to our cluster network, expose the ports so we can connect to it, run it in the background and
tell the docker container to remove itself on termination.

<code class="language-shell">docker run --env-file .env-docker --network advanced-graphql-tutorial -p 4000:4000 -d --rm graphql-tutorial</code>

You should now be able to once again visit http://localhost:4000/graphql and run the same queries, mutations and subscriptions as before.

**Note:** In production, you'll also want to set <code class="language-shell">NODE_ENV=production</code>. This will, however, disable our GraphQL playground
route, so for our dev environments we'll keep it as <code class="language-shell">development</code>.

## Create a cluster with Kubernetes

[Kubernetes](https://kubernetes.io/) is a very popular and well supported container orchestration service. Newer versions of [Docker
Desktop](https://www.docker.com/products/docker-desktop) [includes Kubernetes
support](https://blog.docker.com/2018/07/kubernetes-is-now-available-in-docker-desktop-stable-channel/) which makes local developing and testing for Kubernetes
a bit easier. Sample development config files to set up a Kubernetes environment for this tutorial can be found in the [data-sources/kubernetes] directory.
Before running these files locally, you'll want to be certain to turn off your current docker cluster with <code class="language-shell">npm run
data-stop</code>. You'll also want to update the paths to the volume paths in the "mongo-deployment.yaml" and "mysql-deployment.yaml" files.  While these files
are fine for local development, they should not be used as-is for production.

## Conclusion

Getting our GraphQL server set up to respond to queries, mutations and subscriptions is just one of the steps needed when creating a GraphQL service. Another
important step is to make certain that our service can be deployed and updated in such a way that we can make these updates frequently without causing end-user
impact. Proper handling of process termination is key accomplishing this task in our GraphQL server code.
