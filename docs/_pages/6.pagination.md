---
layout: "step"
title: Pagination
nav_order: 6
permalink: /tutorial/pagination/
---

# Pagination

Depending on your data sets, pagination may just be the most complicated aspect of your GraphQL server implementations to get right. Let's take a look at a few
examples. In our examples, we will be trying to fetch results in pages containing up to 20 results each.

## Interacting with multiple data set types

### Basic Data Sets

These data sets can be described as one that cannot change when requesting one page after another and must follow these constraints:

  1. Sort data field values must but immutable, unique within data set and new inserts must be exclusively incremental or decremental in nature
  2. Values of fields associated with filter parameters must be immutable

An example of this would query our MySQL checkouts table to return all rows ordered by the checkout_date column. The checkout_date column cannot be
changed once it has been written which means that subsequent queries will not alter ordering. The checkout_date column is also incremental in nature as any new
checkout added to the table should have a checkout_date value greater than the current last row of data. For this data set, the only information we would need
to pass to our SQL query to go from page 1 to page 2 would be the checkout_date at which we left off. With that, we could query for the next 20 rows in the
checkouts table and we can be confident that page 2 will always start and end at the exact same rows.

### Dynamic Data Sets

These data sets are more difficult to properly page as updates to the data that drives the results change over time causing inconsistent page results when going
from one page to another and back again. An example of this data set can be made from the same checkouts table in the basic data set example.

Let's say our data set consists of all current checkouts. In other words, we have a filter that returns results where the checkin_date column has a value of
null. In a real world scenario, one user may be running a query against this data set while another user is checking an asset back into the library. If user A
runs a query to get the results of page 1 and then user B checks in an asset listed on page 1, when user A switches to page 2 and the query technique used in
the basic data set is run, page 2 will be missing a result as that result has now been moved to page 1. This can be potentially more harmful when using infinite
scroll loading on the client UI as there is potential that valid results will be excluded altogether. This is known as page drift and can cause great confusion
on the side of your users.

To solve for this problem we want to make certain that our GraphQL query has deterministic results; meaning the same query run at different times results in the
same set of data being returned to the client.

Depending on your database and infrastructure architecture, the optimal solution for you and your GraphQL query will be different from other GraphQL queries.
Covering all possible iterations is beyond the scope of this tutorial, however I will go over one solution that can be applied globally when covering Dynamic
Data Sets below. While this solution will accommodate a great set of use cases, it lacks much of the database specific optimizations available.

## Adding Basic Pagination

### Basic Data Set

Since there are many times when GraphQL queries will return basic data sets, lets start by adding pagination for a basic data set to return all checkout records
with no filters and ordered by checkoutDate.

First, we'll update our typeDef to allow sending some extra parameters used for paging.

- cursor
  - A value that can be sent to our data source as a starting point from which to return results
- next
  - The number of results that we want to return from our cursor
- sort
  - An array of specified fields and the order that they should be sorted

<pre><code class="language-graphql">
# File: src/schema/checkout/checkout.typeDef.js
const typeDef = /* GraphQL */`
  extend type Query {
    checkouts(
      userEmail: String
      assetUpc: String
      <mark>sort: [SortObject!]</mark>
      <mark>cursor: String</mark>
      <mark>next: Int</mark>
    ): [CheckOut]
  }

  ...

  input SortObject {
    field: SortField!
    direction: SortDirection
  }

  enum SortField {
    userEmail
    assetUpc
    checkinDate
    checkoutDate
  }

  enum SortDirection {
    ASC
    DESC
  }
`;
</code></pre>

Now, we'll update our function that generates MySQL query strings to accept and handle these new arguments arguments to append any new WHERE or ORDER BY clauses
that the user has requested.

<pre><code class="language-javascript">
// File: src/schemas/checkout/checkout.models.js
function getQueryStr(args) {
  const queryStr = `select ${mappedQueryFields} from checkouts`;
  const filterKeys = Object.keys(checkoutTableVariablesMap);

  const filters = [];
  let sorters = [{ field: 'checkoutDate', direction: 'ASC' }];
  let limit;
  Object.entries(args).forEach((arg) => {
    const [key, value] = arg;
    switch (key) {
      case 'next':
        limit = parseInt(value, 10);
        break;
      case 'cursor':
        filters.push({
          key: 'checkout_date',
          comparison: args.sort && args.sort[0].field === 'checkoutDate' && args.sort[0].direction === 'DESC' ? '<' : '>',
          value: `STR_TO_DATE(${mysqlDataConnector.escape(value)}, '%Y-%m-%dT%T')`,
        });
        break;
      case 'sort':
        sorters = value.map(sort => ({
          field: mysqlDataConnector.escapeId(sort.field),
          direction: sort.direction === 'DESC' ? 'DESC' : 'ASC',
        }));
        break;
      default:
        if (filterKeys.includes(key)) {
          filters.push({
            key: mysqlDataConnector.escapeId(checkoutTableVariablesMap[key]),
            comparison: value === null ? '' : '=',
            value: value === null ? ' IS NULL' : mysqlDataConnector.escape(value),
          });
        }
        break;
    }
  });

  const filterStr = filters.length > 0 ? `WHERE ${filters.map(filter => `${filter.key}${filter.comparison}${filter.value}`).join(' AND ')}` : '';
  const orderByStr = `ORDER BY ${sorters.map(sort => `${sort.field} ${sort.direction}`).join(', ')}`;
  const limitStr = limit && !Number.isNaN(limit) ? `LIMIT ${limit}` : '';
  return `${queryStr} ${filterStr} ${orderByStr} ${limitStr}`;
}
</code></pre>

Let's test our cursor by requesting the first 10 rows of data.
<pre><code class="language-graphql">
{
  checkouts(next: 10) {
    assetUpc
    checkoutDate
  }
}

#### Result
{
  "data": {
    "checkouts": [
      {
        "assetUpc": "9000000028",
        "checkoutDate": "2000-01-31T10:27:00.000Z"
      },
      {
        "assetUpc": "9000000030",
        "checkoutDate": "2000-01-31T16:21:02.000Z"
      },
      {
        "assetUpc": "9000000029",
        "checkoutDate": "2000-01-31T18:44:23.000Z"
      },
      {
        "assetUpc": "9000000017",
        "checkoutDate": "2000-03-02T09:27:11.000Z"
      },
      {
        "assetUpc": "9000000024",
        "checkoutDate": "2000-03-02T12:12:34.000Z"
      },
      {
        "assetUpc": "9000000023",
        "checkoutDate": "2000-03-02T12:23:06.000Z"
      },
      {
        "assetUpc": "9000000021",
        "checkoutDate": "2000-03-02T13:13:14.000Z"
      },
      {
        "assetUpc": "9000000004",
        "checkoutDate": "2000-03-02T14:01:18.000Z"
      },
      {
        "assetUpc": "9000000001",
        "checkoutDate": "2000-03-02T14:30:17.000Z"
      },
      {
        "assetUpc": "9000000009",
        "checkoutDate": "2000-03-02T15:09:10.000Z"
      }
    ]
  }
}
</code></pre>

We can verify that our paging is working by sending the value of the checkoutDate of the 5th result on this page as the cursor to the next query and only
requesting the next 5 result. These results should match exactly the last five results from the previous query.

<pre><code class="language-graphql">
{
  checkouts(
    next: 5
    cursor: "2000-03-02T12:12:34.000Z"
  ) {
    assetUpc
    checkoutDate
  }
}

#### Result
{
  "data": {
    "checkouts": [
      {
        "assetUpc": "9000000023",
        "checkoutDate": "2000-03-02T12:23:06.000Z"
      },
      {
        "assetUpc": "9000000021",
        "checkoutDate": "2000-03-02T13:13:14.000Z"
      },
      {
        "assetUpc": "9000000004",
        "checkoutDate": "2000-03-02T14:01:18.000Z"
      },
      {
        "assetUpc": "9000000001",
        "checkoutDate": "2000-03-02T14:30:17.000Z"
      },
      {
        "assetUpc": "9000000009",
        "checkoutDate": "2000-03-02T15:09:10.000Z"
      }
    ]
  }
}
</code></pre>

#### A Word on Dates and Timezone support in Javascript

When using a timestamp as a cursor, take extra precaution around timezone support. Frequently, I see many bugs related to timezone support when interacting with
databases. When using a date field as a cursor be absolutely certain that you are handling timezone differences between your database and your Javascript code.
In our example project, our MySQL database is set on UTC time and our connection string specifies that timezone when creating a connection. If possible in your
environment, I highly recommend this setup where all dates are stored and referenced in UTC time. After that, the consumer of the date will have a much easier
time displaying the date to the end user in whatever timezone they wish.

### Dynamic Data Sets

The Basic Data Sets example above restricts proper sorting and cursor to just the checkoutDate field because when querying directly against the MySQL database,
this is the only field that will return deterministic data. We may, however, need to provide the ability to sort by any field requested in our GraphQL query.

In order to accomplish this, we must be able to return a page of data from the same data set as the previously requested page. Given that our data can change in
our database between subsequent fetches for new pages, running a new query for page data directly against our database is not an acceptable option. This can be
accomplished in many ways on various databases. The [oracledb](https://www.npmjs.com/package/oracledb) driver for Node, for example, supports fetching
subsequent rows of data via a [ResultSet](https://oracle.github.io/node-oracledb/doc/api.html#-7-resultset-class). These ResultSets can be kept open on the
graphql server and referenced in subsequent requests for next page data. MongoDB by design is set up to work with paging results as the return value of a
`.fine()` call is a MongoDB Cursor object that can be used to request more and more results from a deterministic data set. The overhead on this approach,
however, is more than most developers may find worth the benefit for GraphQL as it would involved creating the ResultSet or Cursor reference, keeping the
reference open for access across multiple requests, making the reference available to multiple instances of the GraphQL server for auto scaling and properly
closing reference when no more results are going to be requested.

Another approach is simply to cache the results of the initial call for data and then make all subsequent calls for new pages against this data cache. While
this approach does solve our problem with non-deterministic data, there are some pros and cons that you should be aware of.

**Pros**

- Page retrieval speed
  - Since subsequent calls for more data occur against a data cache, paging through results is a very fast procedure
- Only the first level set of data is cached.
  - 2nd level data which is resolved from other data sources continues to be up to date
- Caches are stored in persistent storage and given deterministic IDs
  - Solution is valid even when scaling to multiple load balanced GraphQL servers

**Cons**

- Subsequent calls to pages will result in 1st level data only as updated as the first call
  - We will want to add some sort of mechanism for your users to see the date and time that the data was last refreshed
  - As described above, this is anticipated behavior in many cases, however
- Very large data sets can create large resource consumption
  - If our first query returned hundreds of thousands (or even millions) of rows, that data would be cached
  - Adding a "LIMIT" filter to our query can help with query response times, those filters will not be applied to the original query
  - A messaged to our users such as "Paging limiting to n results" should be added
  - We will want to add a mechanism to remove stale caches
- Because the result data will need to be cached, this may not be an available option in your environment
  - Data access must play a large part in any decision on data display.
  - Your organization may have strict rules around data storage and may require that data be secured at the database layer via non-shared accounts

In any case, you will want to make certain that if using a query cache, the cached results are only available to the same user that initially requested the
data. We will be covering authentication and authorization in MongoDB in a later tutorial.

#### Update the typeDef

Our type definitions will need some updates to handle paging on dynamic data sets.

We will need to add some sort of data for the client to get some information on the current page of data so we'll create a new type called PageInfo.

<pre><code class="language-graphql">
// File: src/schema/checkout/checkout.typeDef.js
type PageInfo {
  queryId: ID!
  currentCursor: String
  startEntryIndex: Int!
  endEntryIndex: Int!
  totalEntries: Int!
  hasPreviousPage: Boolean!
  hasNextPage: Boolean!
}
</code></pre>

Since this data will need to be sent in the response to the client, we are going to create a new query just for paging feeds which returns a new response type
called Feed.

<pre><code class="language-graphql">
// File: src/schema/checkout/checkout.typeDef.js
extend type Query {
  ...
  checkoutFeed(
    userEmail: String
    assetUpc: String
    sort: [SortObject!]
    cursor: String
    next: Int
    queryId: ID
  ): CheckOutFeed
}

type CheckOutFeed {
  pageInfo: PageInfo
  entries: [CheckOut]
}
</code></pre>

Finally, we are going to need to add an "id" field to our CheckOut type. This will be the field that is used to populate the "cursor" to fetch the next page
of results.

<pre><code class="language-graphql">
// File: src/schema/checkout/checkout.typeDef.js
type CheckOut {
  <mark>id: ID!</mark>
  userEmail: String!
  assetUpc: String!
  checkoutDate: String!
  checkinDate: String
}
</code></pre>

#### Add Generic Cache Handling

Now that we have our updated definitions, we'll need to start implementing them. To begin, we are going to create some generic caching functions in a new file
called "cache.js". These caching functions will take care of searching for existing caches, adding new caches and retrieving entries from current caches.

Along with the queryId and cached results, each cache entry will include a "date_created" field. This field can be used to clean out stale caches. One option to
accomplish this is to create a simple service that runs each hour and removes all caches whose data_created value is greater than 24 hours ago. This service
would be completely separate from your GraphQL server.

To store our cache, we will be using MongoDB. In general, I prefer using NoSQL databases for JSON caching and MongoDB is just something with which I am
familiar.

<pre><code class="language-javascript">
// File: src/schema/cache.js
import mongoDataConnector from '../data-connectors/mongodb';

export async function isCacheAvailable(queryId) {
  const db = await mongoDataConnector.getDb();
  const results = await db.collection('querycache').findOne({ _id: queryId }, { _id: 1 });
  return !!results;
}

export async function queryCache(queryId, cursor, count = { $size: '$entries' }) {
  const db = await mongoDataConnector.getDb();
  const mongoResults = await db.collection('querycache')
    .aggregate([
      { $match: { _id: queryId } },
      {
        $project: {
          searchIndex: { $add: [{ $indexOfArray: ['$entries.id', cursor] }, 1] },
          pageInfo: { totalEntries: { $size: '$entries' } },
          entries: '$entries',
        },
      },
      {
        $project: {
          entries: {
            $slice: [
              '$entries',
              { $cond: { if: { $gt: ['$searchIndex', 0] }, then: '$searchIndex', else: 0 } },
              count,
            ],
          },
          pageInfo: {
            totalEntries: '$pageInfo.totalEntries',
            startEntryIndex: { $cond: { if: { $gt: ['$searchIndex', 0] }, then: '$searchIndex', else: 0 } },
          },
        },
      },
    ]).toArray();
  return mongoResults[0];
}

export async function cacheResults(queryId, entries) {
  const db = await mongoDataConnector.getDb();
  await db.collection('querycache').insert({ _id: queryId, date_created: new Date(), entries });
  return entries;
}
</code></pre>

#### Create Resolver Handlers

Next we create a handler function for our resolver. This function will handle checking to see if a cache exists for a particular query, fetch the necessary data
to create the cache if one does not exist and then query from the cache to display the paged results.

When creating a new cache, a queryId is generated. This queryId is not a random value, but rather a hash of the combination of search criteria and the user that
requested the information.

<pre><code class="language-javascript">
// File: src/schema/utils.js
...
import * as crypto from 'crypto';
...
export const getQueryId = (args, user) => {
  const hash = crypto.createHash('md5');
  hash.update(util.inspect(user));
  hash.update(util.inspect(args));
  return hash.digest('hex');
};
</code></pre>

The user information is used to guarantee that another user making the same query does not receive the same results. We do this because depending on your
organizations data access logic, two users may not see the same data for the same query.

When fetching initial data for the cache, we also want to remove any paging variables. The results that will populate the cache should contain all results that
match any other aspects of the request filter. Since the "next" and "cursor" fields are used to page data, we will remove them before requesting data from the
origin data source (MySQL in this case). The "next" and "cursor" data will be passed along to query the cache to return results.

From our typeDef updates we see that the cursor will need to be of type String. When querying the cache, the "id" field for each entry is used to find the
current entry based on the cursor. In our MySQL database, we have a row_id column that is used as an ID, but it is of type Int. Before caching our results, we
will need to coerce this into a string type to allow for equality comparison in MongoDB.

<pre><code class="language-javascript">
// File: src/schema/checkout/checkout.model.js
...
import { getQueryId } from '../utils';
import { isCacheAvailable, queryCache, cacheResults } from '../cache';
...
export const CheckOut = {
  ...
  getCheckoutFeed: async (root, args) => {
    const buildQueryArgs = { ...args };
    if (buildQueryArgs.next) {
      delete buildQueryArgs.next;
    }
    if (buildQueryArgs.cursor) {
      delete buildQueryArgs.cursor;
    }
    const user = 'REPLACE WITH WITH USER FROM CONTEXT IN LATER LESSON';
    const queryId = getQueryId(buildQueryArgs, user);
    const hasCache = await isCacheAvailable(queryId);
    if (!hasCache) {
      const query = getQueryStr(buildQueryArgs);
      const queryResults = await mysqlDataConnector.pool.query(query);
      await cacheResults(queryId, queryResults.map(row => ({ ...row, id: row.id.toString() })));
    }
    const response = await queryCache(queryId, args.cursor, args.next);
    const { entries, pageInfo } = response;
    return {
      pageInfo: {
        queryId,
        currentCursor: args.cursor,
        startEntryIndex: pageInfo.startEntryIndex,
        endEntryIndex: pageInfo.startEntryIndex + entries.length - 1,
        totalEntries: pageInfo.totalEntries,
        hasPreviousPage: pageInfo.startEntryIndex > 0,
        hasNextPage: pageInfo.startEntryIndex + entries.length < pageInfo.totalEntries,
      },
      entries,
    };
  },
  ...
}
</code></pre>

The last step is simply to hook up our resolver handler to our resolver so GraphQL knows how to handle requests for the checkoutFeed.

<pre><code class="language-javascript">
import CheckOut from './checkout.model';

export default {
  Query: {
    checkouts: CheckOut.getCheckouts,
    checkoutFeed: CheckOut.getCheckoutFeed,
  },
  ...
}
</code></pre>

#### Validate Paging

Let's run a basic query on our CheckoutFeed to get the first 10 entries.

<pre><code class="language-graphql">
{
  checkoutFeed (
    next: 10
  ) {
    entries {
      id
      assetUpc
      checkinDate
      checkoutDate
      patron {
        email
        firstName
        lastName
      }
    }
    pageInfo {
      queryId
      currentCursor
      startEntryIndex
      endEntryIndex
      totalEntries
      hasPreviousPage
      hasNextPage
    }
  }
}
</code></pre>

Along with our first 10 entries, we should see the new pageInfo field that we requested. This gives us some information that our client can use to create
pagination controls for end user as well as query for the next result.

<pre><code class="language-json">
"pageInfo": {
  "queryId": "8c165b3eac2af474528e4970dd1473c2",
  "currentCursor": null,
  "startEntryIndex": 0,
  "endEntryIndex": 9,
  "totalEntries": 421,
  "hasPreviousPage": false,
  "hasNextPage": true
}
</code></pre>

Now lets test our cursor by requesting the next 5 results starting from the ID for the 5th user in the list. This should give us the 2nd set of 5 users from our
original query.

<pre><code class="language-graphql">
{
  checkoutFeed (
    next: 5,
    cursor: "261"
  ) {
    entries {
      id
      assetUpc
      checkinDate
      checkoutDate
      patron {
        email
        firstName
        lastName
      }
    }
    pageInfo {
      queryId
      currentCursor
      startEntryIndex
      endEntryIndex
      totalEntries
      hasPreviousPage
      hasNextPage
    }
  }
}
</code></pre>

Unlike the pagination in the basic example, we can utilize the sort and other filter criteria to the dynamic paging and paging will still work since the paging
is based off of a data set that was cached on the first call. When adjusting the filter/sort criteria, it may be best from a user point of view to remove the
cursor from the initial query and reset any display back to page 1. Our solution does not require this, but the cursor from one query may be on a different page
than another query which may cause confusion on the side of the user.

<pre><code class="language-graphql">
{
  checkoutFeed (
    next: 5
    sort: [
      { field: assetUpc, direction: ASC }
      { field: checkoutDate, direction: DESC }
    ]
  ) {
    entries {
      id
      assetUpc
      checkinDate
      checkoutDate
      patron {
        email
        firstName
        lastName
      }
    }
    pageInfo {
      queryId
      currentCursor
      startEntryIndex
      endEntryIndex
      totalEntries
      hasPreviousPage
      hasNextPage
    }
  }
}
</code></pre>

<pre><code class="language-graphql">
{
  checkoutFeed (
    next: 5
    cursor: "11"
    sort: [
      { field: assetUpc, direction: ASC }
      { field: checkoutDate, direction: DESC }
    ]
  ) {
    entries {
      id
      assetUpc
      checkinDate
      checkoutDate
      patron {
        email
        firstName
        lastName
      }
    }
    pageInfo {
      queryId
      currentCursor
      startEntryIndex
      endEntryIndex
      totalEntries
      hasPreviousPage
      hasNextPage
    }
  }
}
</code></pre>
