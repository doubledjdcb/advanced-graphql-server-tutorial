---
layout: "step"
title: Add Another Data Source
nav_order: 4
permalink: /tutorial/add-mongo/
---

# Add Another Data Source - MongoDB

In this tutorial, we’ll be adding a second data source that contains information that we can use to augment the data available in our CheckOut type. This can be
very useful if you are in an organization with multiple fragmented data sources each containing different aspects of the information needed for a particular
project. Using GraphQL, you can create a single point of data access for app development needs.

This is also a technique that can be used if you are in a project to retire particular data sources. In a multi-stage approach, you can first place the GraphQL
server in front of the legacy data service and migrate your clients over to the GraphQL server. Once that is complete, replacing the existing data source with
an alternative would be as easy as updating individual resolvers as the data migration progresses.

We will be using MongoDB as our secondary data source in this example and interacting with our Mongo database with the official
[node-mongodb](https://www.npmjs.com/package/mongodb) package. The concepts that you will learn in this tutorial can be transferred to other alternative data
sources as well. One example may be using an existing REST api with a package that implements the [Fetch
API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API).

## Create the MongoDB Connector

First, we'll start by adding mongodb with `npm install mongodb`.

Second, we'll need to update our environment variables to support our mongodb connection.

<pre><code class="language-javascript">
// File: src/env.js
...
const environmentVariables = {
  ...
  MONGO_URL: process.env.MONGO_URL,
  MONGO_DATABASE: process.env.MONGO_DATABASE,
  MONGO_USER: process.env.MONGO_USER,
  MONGO_PASSWORD: process.env.MONGO_PASSWORD,
  MONGO_CONNECTION_POOL_SIZE: process.env.MONGO_CONNECTION_POOL_SIZE ? parseInt(process.env.MONGO_CONNECTION_POOL_SIZE, 10) : 10,
};
...
</code></pre>
<br />

<pre><code class="language-javascript">
// File: src/env.js
...
MONGO_URL=mongodb://localhost:27017
MONGO_USER=graphql
MONGO_PASSWORD=graphqlpw
MONGO_DATABASE=central-library
MONGO_CONNECTION_POOL_SIZE=10
</code></pre>
<br />

Just like our MySQL implementation, we’ll want to create a data connector that can handle connection pooling and querying. Since each database is a little
different in how these things are managed, our data-connectors should be set up to be specific to each database. For MongoDB, the big difference from MySQL is
how connection pooling is handled.  We’ll add a little extra code in our data connector to manage a connection and re-establish a connection if it drops.

<pre><code class="language-javascript">
// File: src/data-connectors/mongodb.js
import { MongoClient, ObjectID } from 'mongodb';
import env from '../env';


export function convertStringToID(idStr) {
  return new ObjectID(idStr);
}

class MongoService {
  constructor() {
    this.client = new MongoClient(env.MONGO_URL, {
      useNewUrlParser: true,
      authSource: env.MONGO_DATABASE,
      auth: {
        user: env.MONGO_USER,
        password: env.MONGO_PASSWORD,
      },
      poolSize: env.MONGO_CONNECTION_POOL_SIZE,
    });
    this.isConnecting = false;
  }

  async getDb() {
    await this.getConnection();
    const db = this.client.db(env.MONGO_DATABASE);
    return db;
  }

  async getConnection() {
    if (this.isConnecting) {
      await this.waitForConnection();
    }
    if (!this.isClientConnected) {
      this.isConnecting = true;
      await this.client.connect();
      this.isConnecting = false;
    }
  }

  async closeConnection() {
    if (this.isClientConnected) {
      await this.client.close();
    }
  }

  get isClientConnected() {
    return this.client.isConnected();
  }

  /**
   * If our mongo connection drops, wait for it to be re-established before processing more queries
   */
  waitForConnection() {
    return new Promise((resolve) => {
      const checkConnected = () => {
        if (this.isClientConnected) {
          resolve();
        } else {
          setTimeout(() => checkConnected(resolve), 100);
        }
      };
      checkConnected();
    });
  }
}

export default new MongoService();
</code></pre>

## Create the Type Definitions

Now that we have our new data connector, we can create our typeDef for the information that we will be gathering from our MongoDB database. This database
contains information on both patrons of the library and details of the books in the library so we will create two new directories and files to hold our new
type definitions.

<pre><code class="language-javascript">
// File: src/schema/patron/patron.typeDef.js
const typeDef = /* GraphQL */`
  type Patron {
    id: ID!
    yearRegistered: Int!
    firstName: String!
    lastName: String!
    email: String!
    phoneCell: String!
  }
`;

export default typeDef;
</code></pre>
<br />

<pre><code class="language-javascript">
// File: src/schema/book/book.typeDef.js
const typeDef = /* GraphQL */`
  type Book {
    id: ID!
    title: String!
    author: String!
    yearPublished: Int!
    genre: String!
    isbn13: String!
    copies: [BookCopy]
  }

  type BookCopy {
    libraryUPC: String!
    condition: BookCondition!
  }

  enum BookCondition {
    good
    fair
    damaged
  }
`;

export default typeDef;
</code></pre>
<br />

## Write MongoDB Query Code

Next we will define our data fetching methods. Similar to our functions to fetch data from our MySQL data source, we will take any arguments passed to our query
and use them to construct the query to be sent to the data source.

### Optimizing for Data Manipulation Load

One large difference between MongoDB and MySQL is how result fields can be renamed before returning the results to the GraphQL server which leads us to a short
side note on query optimization and where to put processing load.  Both our GraphQL server and our MongoDB server have the ability to translate the field names
specified in the data source to the corresponding field names in our GraphQL types. Where we choose to execute the logic used to rename those fields depends
greatly on resources available to each part of the system. If your database has enough compute resources and can handle the additional load without causing
performance issues for other users of the database, it is best to hand any computational tasks to the database. Typically, databases have optimization
algorithms which can handle various types of data manipulations with greater efficiency than can be done on a Node.js server. This may extend far beyond simple
renaming of fields into much more complex data manipulation tasks. I strongly encourage you to learn more about the capabilities of the data sources you are
using if you are unfamiliar with them. Taking the time to become an expert in those capabilities may allow for huge gains in GraphQL api speed.  In this
tutorial, we will be using both database side renaming as well as javascript mapping in examples to show the differences.

<pre><code class="language-javascript">
// File src/schemas/patron/patron.constants.js
// For use with database field name translations
export const projectFieldMapping = {
  id: { $toString: '$_id' },
  firstName: '$first_name',
  lastName: '$last_name',
  email: '$email',
  yearRegistered: '$year_registered',
  phoneCell: '$phone_cell',
};
</code></pre>
<br />

<pre><code class="language-javascript">
// File src/schemas/patron/patron.model.js
import mongoDataConnector, { convertStringToID } from '../../data-connectors/mongodb';
import { projectFieldMapping } from './patron.constants';

const Patron = {
  getPatrons: async (root, args) => {
    const db = await mongoDataConnector.getDb();
    const match = {};
    if (args.yearRegistered) {
      match.year_registered = args.yearRegistered;
    }

    const cursor = db.collection('patrons').aggregate([
      { $match: match },
      { $project: projectFieldMapping },
    ]);
    if (args.limit) {
      cursor.limit(args.limit);
    }
    const patrons = cursor.toArray();
    return patrons;
  },
};

export default Patron;
</code></pre>
<br />

<pre><code class="language-javascript">
// File src/schema/book/book.model.js
import mongoDataConnector, { convertStringToID } from '../../data-connectors/mongodb';

// For Node.js server field name translation
function mongoBookToGraphQLType(doc) {
  return {
    id: doc._id.toString(),
    title: doc.title,
    author: doc.author,
    yearPublished: parseInt(doc.yearPublished, 10),
    genre: doc.genre,
    isbn13: doc.isbn_13,
    copies: doc.copies,
  };
}

const Book = {
  getBooks: async () => {
    const db = await mongoDataConnector.getDb();
    const books = await db.collection('books').find({}).toArray();
    return books.map(mongoBookToGraphQLType);
  },
};

export default Book;
</code></pre>
<br />

Once our data fetching functions are complete, we can extend the Query type in our typeDefs and expand our resolvers to implement the Query methods for both
books and patrons.

<pre><code class="language-graphql">
// File: src/schema/patron/patron.typeDef.js
const typeDef = /* GraphQL */`
  extend type Query {
    patrons: [Patron]!
  }
  ...
`;
</code></pre>
<br />

<pre><code class="language-graphql">
// File: src/schema/book/book.typeDef.js
const typeDef = /* GraphQL */`
  extend type Query {
    books: [Book]
  }
  ...
`;
</code></pre>
<br />

<pre><code class="language-javascript">
// File: src/schema/book/book.resolver.js
import Book from './book.model';
export default {
  Query: {
    books: Book.getBooks,
  },
};
</code></pre>
<br />

<pre><code class="language-javascript">
// File: src/schema/patron/patron.resolver.js
import Patron from './patron.model';
export default {
  Query: {
    patrons: Patron.getPatrons,
  },
};
</code></pre>
<br />

Finally, we register our new typeDef and resolver with our schema.

<pre><code class="language-javascript">
// File: src/schema/index.js
import patronResolver from './patron/patron.resolver';
import patronTypeDef from './patron/patron.typeDef';
import bookResolver from './book/book.resolver';
import bookTypeDef from './book/book.typeDef';
...
const typeDefs = [
  baseTypeDef,
  checkoutTypeDef,
  <mark>patronTypeDef,</mark>
  <mark>bookTypeDef,</mark>
];

const resolvers = merge(
  checkoutResolver,
  <mark>patronResolver,</mark>
  <mark>bookResolver,</mark>
);

const schema = makeExecutableSchema({
  typeDefs,
  resolvers,
});
</code></pre>
<br />

We can test our new queries by heading back to [http://localhost:4000/graphql](http://localhost:4000/graphql) and running queries for both books and patrons.

<pre><code class="language-graphql">
{
  books {
    title
  }
}
</code></pre>
<br />

<pre><code class="language-graphql">
{
  patrons {
    firstName
    lastName
  }
}
</code></pre>

## Augment Other Types

With our query functionality working, we can start to augment the type created for data from our MySQL data source with data from our MongoDB data source.  In
our checkout.typeDef.js file we will add both a "patron" field and a "book" to the CheckOut type and give them the types Patron and Book that we specified in
our typeDef files respectively. In keeping with our module project structure, we will define the extended CheckOut type from the module for that particular
data. Since all checkouts will be associated with a patron and a book, we can make these fields non nullable.

<pre><code class="language-graphql">
// File: src/schema/patron/patron.typeDef.js
const typeDef = /* GraphQL */`
  ...
  extend type CheckOut {
    patron: Patron!
  }
`;
</code></pre>
<br />

<pre><code class="language-graphql">
// File: src/schema/book/book.typeDef.js
const typeDef = /* GraphQL */`
  ...
  extend type CheckOut {
    book: Book!
  }
`;
</code></pre>
<br />

Next we need to tell GraphQL how it should resolve the patron and book fields when a CheckOut is requested. Again, to keep with our module project, we specify
those in the resolver files for the data associated with the module. We'll create a new getPatronByEmail method on the Patron model and a
getBookByCopyLibraryUpc method on the Book model.

<pre><code class="language-javascript">
// File: src/schema/patron/patron.model.js
const Patron = {
  ...
  getPatronByEmail: async (email) => {
    const db = await mongoDataConnector.getDb();
    const cursor = await db.collection('patrons').aggregate([
      { $match: { email } },
      { $project: projectFieldMapping },
    ]).limit(1);
    const patron = await cursor.next();
    await cursor.close();
    return patron;
  },
  ...
};
</code></pre>
<br />

<pre><code class="language-javascript">
// File: src/schema/book/book.model.js
const Book = {
  ...
  getBookByCopyLibraryUpc: async (libraryUpc) => {
    const db = await mongoDataConnector.getDb();
    const book = await db.collection('books').findOne({ 'copies.libraryUPC': libraryUpc });
    return book ? mongoBookToGraphQLType(book) : null;
  },
  ...
};
</code></pre>
<br />

Now we'll create a resolver method in the associated modules resolver files to implement our new type field. These new resolvers will be merged in with the rest
of the CheckOut resolvers using `lodash.merge` in when creating the schema.

<pre><code class="language-javascript">
// File: src/schema/book/book.resolver.js
export default {
  Query: {
    book: Book.getBook,
    books: Book.getBooks,
  },

  <mark>CheckOut: {
    book: checkout => Book.getBookByCopyLibraryUpc(checkout.assetUpc),
  },</mark>
};
</code></pre>
<br />

<pre><code class="language-javascript">
// File: src/schema/patron/patron.resolver.js
export default {
  Query: {
    patron: Patron.getPatron,
    patrons: Patron.getPatrons,
  },

  <mark>CheckOut: {
    patron: checkout => Patron.getPatronByEmail(checkout.userEmail),
  },</mark>
};

</code></pre>
<br />

Once all of that is saved and our server restarted we can go back to [http://localhost:4000/graphql](http://localhost:4000/graphql) and verify that we can now
request the patron for each checkout along with information for that patron.

<pre><code class="language-graphql">
{
  checkouts(userEmail: "dbernath27@jalbum.net") {
    assetUpc
    checkoutDate
    checkinDate
    book
    {
      title
      author
    }
    patron {
      id
      email
      firstName
      lastName
    }
  }
}
</code></pre>
<br />

## Augment All Connecting Data Types

We can also augment our books query to allow us to return all of the current checkouts for any copy of a particular book. In our checkout.typeDef.js file we can
extend the BookCopy type and add a checkoutHistory field which returns an array of CheckOuts to show which patrons currently have any copy of this particular
book. We can also specify that this field can take a filter parameter to only include CheckOut records where the copy has not yet been checked back in. Go ahead
and try updating your project to add these new features on your own. If you get stuck, you can see sample implementations in the
[step-4-separate-data-source](https://github.com/hydrateio/advanced-graphql-server-tutorial/tree/step-4-separate-data-source) of the code repository.

Our new query may look like this to find all currently checked out copies of a particular book along with the information of the patron who has the book checked
out.

<pre><code class="language-graphql">
{
  book(id:"5c336c48459025275babfa73") {
    id
    author
    title
    isbn13
    copies {
      libraryUPC
      condition
      checkoutHistory(currentCheckoutsOnly: true) {
        checkoutDate
        checkinDate
        patron {
          firstName
          lastName
        }
      }
    }
  }
}
</code></pre>
<br />

To complete our augmentation, we’ll also augment our patron query to allow us to query for everything the patron has checked out. By altering the Patron type
and adding adding a field resolver.  After that, we can search for a patron and list the books they currently have checked out.

<pre><code class="language-graphql">
{
  patron(email: "adampier1@jalbum.net") {
    email
    firstName
    lastName
    yearRegistered
    checkOuts(currentCheckoutsOnly: true) {
      assetUpc
      checkoutDate
      checkinDate
      book {
        title
      }
    }
  }
}
</code></pre>
<br />

We now have our GraphQL server set up to query multiple data sources and return consolidated data back to the clients. At this point, the sources systems for
data becomes an implementation detail and can be swapped out at any point without the client being affected by those changes.
