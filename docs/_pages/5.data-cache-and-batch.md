---
layout: "step"
title: Cache and Batch Requests
nav_order: 5
permalink: /tutorial/cache-and-batch/
---

# Cache and Batch Requests for Data

In this tutorial we will go introduce a data request batching and caching optimization. Now that we have data fields from one data source augmenting the results
of data from another data source a single query to our GraphQL server may result in multiple queries to our secondary data source as those fields are being
resolved. In our library, an example of this would be when we query for all checkouts. Currently, our first query to our MySQL database will return an array of
checkouts. As the GraphQL iterates over those checkouts to populate the book and patron data a new query is run against the MongoDB for each CheckOut entry.
This results in the exact same query being run multiple times if a single patron has had multiple checkouts.

## Profiling MongoDB Queries

If you’re new to MongoDB and are looking to optimize queries, you will be interested in MongoDB’s profiling capabilities along with the <code
class="language-javascript">query.explain()</code> method. MongoDB also has 3 profiling levels and for our example we’ll be setting the highest profiling using
the <code class="language-javascript">db.setProfilingLevel()</code> method and then querying against the system.profile collection to verify our batch requests
are hitting the database as expected.

{% raw %}
<pre><code class="language-bash">
$ docker ps --filter name=mongo --format "table {{.ID}}\t{{.Names}}"
CONTAINER ID        NAMES
<mark>7f3494113567</mark>        data-sources_mongo_1

$ docker exec -it <mark>7f3494113567</mark> /bin/bash
root@7f3494113567:/# mongo
Welcome to the MongoDB shell.
> use admin
switched to db admin
> db.auth('root', 'rootpw')
1
> use central-library
switched to db central-library
> db.setProfilingLevel(2)
{ "was" : 0, "slowms" : 100, "sampleRate" : 1, "ok" : 1 }
</code></pre>
{% endraw %}

With our profiling level set to 2, we can run the following graphql query.

<pre><code class="language-graphql">
{
  checkouts {
    assetUpc
    checkoutDate
    checkinDate
    patron {
      id
      email
      firstName
      lastName
    }
  }
}</code></pre>

In our MongoDB prompt we query number calls to the database by the graphql user and see that 421 calls to the database were made.

<pre><code class="language-javascript">
db.getCollection('system.profile').find({user: 'graphql@central-library'}, {command:1, ts: 1}).count()
</code></pre>

Furthermore, if we search for all queries for a specific email we can see the exact same query was made 5 times for email dbernath27@jalbum.net

<pre><code class="language-javascript">
db.getCollection('system.profile').find({"command.pipeline.$match.email": "dbernath27@jalbum.net"}).count()
</code></pre>

If at any point we want to clear out our profile data, we can set the profiling level back to 0, drop the profile collection and reset the profiling level back
to 2 in our MongoDB command prompt.

<pre><code class="language-javascript">
db.setProfilingLevel(0); db.system.profile.drop(); db.setProfilingLevel(2);
</code></pre>

## Batching Data Requests

Since the results of our GraphQL query are not returned to the client until all data has been resolved, it would be beneficial if the query to fetch patron
information could wait until we knew all of the patrons that were to be queried and then make a single call to our MongoDB database.

To accomplish this, we are going to bring in the [dataloader](https://www.npmjs.com/package/dataloader) library to our project and use that to batch our
multiple requests into a single request.

When adding dataloader to the project, we will first need to create a new data fetching functions that will accept an array of search criteria and return an array of result data in the exact same
order that was requested. If no information is found for a particular item in the search array, a null object must exist at the corresponding index of the returned array.

Let's add dataloader functionality for our patron data. In our patrons.model.js file we create a new function called <code
class="language-javascript">getPatronsByEmails()</code>. This function takes an array of email addresses, runs a query against the MongoDB database and returns
all patrons that have an email address specified in the array. We then map the results in the order of the emails array argument and return the results.

<pre><code class="language-javascript">
// File: src/schema/patron/patron.model.js
const Patron = {
  ...
  getPatronsByEmails: async (emails) => {
    const db = await mongoDataConnector.getDb();
    const patrons = await db.collection('patrons').aggregate([
      { $match: { email: { $in: emails } } },
      { $project: projectFieldMapping },
    ]).toArray();
    const patronMap = patrons.reduce((map, patron) => {
      map[patron.email] = patron;
      return map;
    }, {});
    return emails.map(email => patronMap[email]);
  },
  ...
};
</code></pre>

Now we need to create the data loaders to call the new data fetching method when all of the patron information has been identified. In a new
patron.data-loaders.js file, we import both the dataloader library and the getPatronsByEmails function and then export another function that when called returns
an object with the new DataLoader instance that passes the list of unique email addresses loaded by the data loader to our new getPatronsByEmails function.  

<pre><code class="language-javascript">
// File: src/schema/patron/patron.data-loaders.js
import DataLoader from 'dataloader';
import Patron from './patron.model';

export default () => ({
  patronsByEmails: new DataLoader(keys => Patron.getPatronsByEmails(keys)),
});
</code></pre>

Next we need to set up our Apollo Server to load this new data loader into the context created for every request. In our src folder we can create a new
“context” folder and within that a “data-loaders.js” file. The data-loaders file will have the sole purpose of aggregating our data loaders from where are the
defined in the project into a single loaders object. In this file we will import our new default export from our patron.data-loaders file and re-export the data
loaders created when run.

<pre><code class="language-javascript">
// File: src/context/data-loaders.js
import getPatronDataLoaders from '../schema/patron/patron.data-loaders';

const getDataLoaders = () => ({
  ...getPatronDataLoaders(),
});
export default getDataLoaders;
</code></pre>

As an abstraction, since much more can be placed on the context, we will create an index.js file in the context folder that will aggregate all properties that
we may want to add to the context. For now will just just add a “loaders” property with the return object from our data-loaders default export and import that
into our main index.js file passing it to the ApolloServer.

<pre><code class="language-javascript">
// File: src/context/index.js
import getDataLoaders from './data-loaders';

export default () => ({
  loaders: getDataLoaders(),
});
</code></pre>

<pre><code class="language-javascript">
// File: src/index.js
...
import context from './context';

const server = new ApolloServer({
  schema,
  <mark>context</mark>,
});
...
</code></pre>

Now that we have our dataloader attached to the Apollo Server context, we can reference it in our data resolvers. In the patron.resolvers.js file we can update
our “patron” field resolver for the extended CheckOut type resolver to call our new data loader rather than the singular getPatronByEmail function we were
using. As a clean-up task, we'll also delete the getPatronByEmail import from this file.  

<pre><code class="language-javascript">
// File: src/schema/patron/patron.resolver.js
import Patron from './patron.model';

export default {
  ...

  CheckOut: {
    patron: (checkout, args, context) => context.loaders.patronsByEmails.load(checkout.userEmail),
  },
};
</code></pre>

To verify our work, we can reset the profiler data in the MongoDB prompt.
<pre><code class="language-javascript">
db.setProfilingLevel(0); db.system.profile.drop(); db.setProfilingLevel(2);
</code></pre>

Then run the exact same query we had before in the GraphQL Playground.
<pre><code class="language-graphql">
{
  checkouts {
    assetUpc
    checkoutDate
    checkinDate
    patron {
      id
      email
      firstName
      lastName
    }
  }
}
</code></pre>

And finally, query the MongoDB profile collection to verify our data loader is optimizing our query in the MongoDB prompt.

<pre><code class="language-javascript">
db.getCollection('system.profile').find({user: 'graphql@central-library'}, {command:1, ts: 1}).count()
</code></pre>

If we replace the last <code class="language-javascript">.count()</code> from the query with <code class="language-javascript">.pretty()</code> we can see the
entire formatted command sent to the MongoDB database and notice that each email address is listed once in the command filter array.

<pre><code class="language-javascript">
> db.getCollection('system.profile').find({user: 'graphql@central-library'},{command:1, ts: 1}).pretty()
{
  "command" : {
    "aggregate" : "patrons",
    "pipeline" : [
      {
        "$match" : {
          "email" : {
            "$in" : [
              "dbernath27@jalbum.net",
              "lholyland21@blog.com",
              "ggenery22@elegantthemes.com",
              ...
              "lpaxeford1u@blogger.com"
            ]
          }
        }
      },
      {
        "$project" : {
          "id" : {
            "$toString" : "$_id"
          },
          "firstName" : "$first_name",
          "lastName" : "$last_name",
          "email" : "$email",
          "yearRegistered" : "$year_registered",
          "phoneCell" : "$phone_cell"
        }
      }
    ],
    "cursor" : {

    },
    "lsid" : {
      "id" : UUID("c1a05f5d-b438-4a8c-a102-a29459316461")
    },
    "$db" : "central-library"
  },
  "ts" : ISODate("2018-12-18T21:26:46.578Z")
}
</code></pre>

Now lets clear out our profiler again and do a query on books and populate the checkout history for each copy of each book.

<pre><code class="language-graphql">
{
  books {
    id
    author
    title
    isbn13
    copies {
      libraryUPC
      condition
      checkoutHistory(currentCheckoutsOnly: true) {
        checkoutDate
        checkinDate
        patron {
          firstName
          lastName
        }
      }
    }
  } 
}
</code></pre>

Checking our MongoDB profiler, we see a bunch of individual requests for patrons again.

<pre><code class="language-javascript">
db.getCollection('system.profile').find({user: 'graphql@central-library'},{command:1, ts: 1}).pretty()
</code></pre>

So what’s happening here? This query uses the same data loader as the other query, however the patrons are being resolved as a nested field of another field
that had been resolved previously. So while results from previous queries to fetch patron information by email are cached and not made to the MongoDB again,
queries to fetch new patron data are not being batched for the entire GraphQL query, but rather only for each instance of the book copy. To fix this, we’ll need
to add dataloaders to each level that resolves data.

Since the next level up in our query is the checkoutHistory, let’s create a data loader to batch request CheckOuts for book copies. In the
checkout.model.js file, create a new function to query multiple checkouts from a list of asset UPCs.

<pre><code class="language-javascript">
// File: src/schema/checkout/checkout.model.js
const CheckOut = {
  ...
    getCheckoutsByAssetUpcs: async (assetUpcs, restrictToCurrentlyCheckedOut) => {
    let filter = 'asset_upc IN (?)';
    if (restrictToCurrentlyCheckedOut) {
      filter = `${filter} AND checkin_date IS NULL`;
    }
    const queryStr = `SELECT ${mappedQueryFields} FROM checkouts WHERE ${filter} ORDER BY checkoutDate DESC`;
    const query = mysqlDataConnector.format(queryStr, [assetUpcs]);
    const queryResults = await mysqlDataConnector.pool.query(query);
    const assetMap = queryResults.reduce((map, result) => {
      if (!map[result.assetUpc]) {
        map[result.assetUpc] = [];
      }
      map[result.assetUpc].push(result);
      return map;
    }, {});
    return assetUpcs.map(upc => assetMap[upc]);
  },
  ...
};
</code></pre>

Since our new function to batch fetch checkouts takes a filter argument, we’re going to need to modify our data loading technique just a bit. Instead of passing
a single key to our data loader's <code class="language-javascript">load()</code> method, we’ll need to pass both the key and the arguments passed from the
client query. The data loaders need unique keys when determining the data that is to be cached and batched, so we’ll create a small helper method to pass as an
option when creating a new DataLoader that generates a unique key. The <code class="language-javascript">inspect()</code> method from node’s util library is
ideal for this task. Next we create another small helper method that merges the various keys and args passed to the loader load method into a single keys array
and arguments object. We'll be creating those in a new file so they can be easily re-used in other data-loaders.

<pre><code class="language-javascript">
// File: src/schema/utils.js
import * as util from 'util';

export const loaderOptions = {
  cacheKeyFn: key => util.inspect(key),
};

export const mergeKeys = arr => arr.reduce(
  (map, obj) => {
    map.keys.push(obj.key);
    map.args = { ...map.args, ...obj.args };
    return map;
  },
  { keys: [], args: {} },
);
</code></pre>

Once we have those helper function we can create a new file called "checkout.data-loaders.js" and import the dataloader library, our utility functions and our
new function for getting checkouts from multiple assets UPCs. Our default export is a function that returns new instances of the new DataLoader which we will
re-export in our context/index.js file.

<pre><code class="language-javascript">
// File: src/schema/checkout/checkout.data-loaders.js
import DataLoader from 'dataloader';
import { loaderOptions, mergeKeys } from '../utils';
import { getCheckoutsByAssetUpcs } from './checkout.model';

const getCheckoutsByAssetUpcsLoaderFn = async (data) => {
  const { keys, args } = mergeKeys(data);
  const patrons = await getCheckoutsByAssetUpcs(keys, args.currentCheckoutsOnly);
  return patrons;
};

export default () => ({
  checkoutsByAssetUpcs: new DataLoader(keys => getCheckoutsByAssetUpcsLoaderFn(keys), loaderOptions),
});
</code></pre>

<pre><code class="language-javascript">
// File: src/context/data-loaders.js
import getCheckoutDataLoaders from '../schema/checkout/checkout.data-loaders';

const getDataLoaders = () => ({
  ...getPatronDataLoaders(),
  ...getCheckoutDataLoaders(),
});
export default getDataLoaders;
</code></pre>

Now back in our checkout.resolver.js file, we can swap out our old checkoutHistory resolver for our new data loader and pass in an object to our data loader
load method that includes by a key property for the libraryUPC as well as the args property from the client query.

<pre><code class="language-javascript">
// File: src/schema/checkout/checkout.resolver.js
BookCopy: {
  checkoutHistory: (copy, args, context) => context.loaders.checkoutsByAssetUpcs.load({ key: copy.libraryUPC, args }),
},
</code></pre>

After a server restart we can verify that now our CheckOuts are being batched for both our checkouts and books queries by clearing out the mongo profile
collection, re-running the same query in the GraphQL Playground and then re-querying our profile collection. The result of our MongoDB query should now show a
single query was run with all email addresses batched into that single request.

The last step is to change over all resolver methods that query data from multiple sources to use DataLoaders and attach those loaders to our context. Go ahead
and try create data loaders to resolve books details in the CheckOut type as well at the checkOuts field of the Patron type.

If you get stuck, you can reference the
["step-5-cache-and-batch" branch](https://github.com/hydrateio/advanced-graphql-server-tutorial/tree/step-5-cache-and-batch) of the project repository for
sample solutions.
